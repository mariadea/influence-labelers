{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "matplotlib.rcParams['mathtext.fontset'] = 'cm'\n",
    "matplotlib.rcParams.update({'font.size': 15})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../results/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"../results/mimic_1_log_p1=6_p2=0.95_p3=0.02.pkl\"\n",
    "dataset = \"../data/triage_scenario_1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(dataset, index_col = [0, 1])\n",
    "covariates, target = data.drop(columns = ['D', 'Y1', 'Y2', 'YC', 'acuity', 'nurse']), data[['D', 'Y1', 'YC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pickle.load(open(result_file, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(results, target, p):\n",
    "    \"\"\"\n",
    "        Evaluate dictionary of results\n",
    "\n",
    "    Args:\n",
    "        results (_type_): _description_\n",
    "        target ()\n",
    "        p (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: Dictionary of results\n",
    "    \"\"\"\n",
    "    evaluation = {}\n",
    "    \n",
    "    for i, result in enumerate(results): \n",
    "        eval = {\n",
    "            '$f_Y$': compute_metrics(result['Observed'], target, p),\n",
    "            '$f_h$': compute_metrics(result['Human'], target, p),\n",
    "            '$f_\\mathcal{A}$': compute_metrics(result['Amalgamation'], target, p),\n",
    "            '$f_{hyb}$': compute_metrics(result['Hybrid'], target, p),\n",
    "        }\n",
    "        try: eval['Observed Negative Outcome'] = {(tar, 'TNR'): (1 - target[tar].loc[results[0].index][covariates.loc[results[0].index].anchor_age == 1]).mean() for tar in target.columns}\n",
    "        except: pass\n",
    "        evaluation[i] = pd.DataFrame.from_dict(eval)\n",
    "    \n",
    "    evaluation = pd.concat(evaluation)\n",
    "    evaluation.index.rename(['Fold', 'Outcome', 'Metric'], inplace = True)\n",
    "    return evaluation \n",
    "\n",
    "def compute_metrics(predictions, target, p):\n",
    "    metrics = {}\n",
    "\n",
    "    for tar in target.columns:\n",
    "        metrics[(tar, 'AUC-ROC')] = roc_auc_score(target[tar].loc[predictions.index], predictions)\n",
    "        try:\n",
    "            smallest = predictions.nsmallest(n = int(p * len(target)))\n",
    "            old_test = covariates[covariates.anchor_age == 1].index.intersection(predictions.index)\n",
    "            oldest   = smallest.loc[smallest.index.intersection(old_test)]\n",
    "            metrics[(tar, 'TNR')] = len(oldest) / len(old_test)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_p = {p: evaluate(results, target, p) for p in [0.5]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.5\n",
    "metric = 'AUC-ROC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['tab:green', 'tab:red', 'tab:blue', 'tab:orange', 'tab:grey']\n",
    "patterns = ['/', '-', '\\\\', '.', '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = evaluation_p[p].groupby(['Metric', 'Outcome']).mean()\n",
    "std = evaluation_p[p].groupby(['Metric', 'Outcome']).std()\n",
    "\n",
    "ax = mean.loc[metric].dropna(axis = 1, how = 'all').plot.bar(edgecolor = 'white', width = 0.8, figsize = (10, 5), yerr = std.loc[metric].dropna(axis = 1, how = 'all'),\n",
    "                            color = colors)\n",
    "\n",
    "# Add hatch\n",
    "hue = mean.loc[metric]\n",
    "hatches = [p for p in patterns for _ in range(len(hue))]\n",
    "for i, (bar, hatch) in enumerate(zip(ax.patches, hatches)):\n",
    "    bar.set_hatch(hatch)\n",
    "\n",
    "# Add separation lines\n",
    "lines = np.array([bar.get_x() for bar in ax.patches])\n",
    "for line in lines[-len(hue):-1] + ((lines[1:len(hue)] - lines[-len(hue):-1] + bar.get_width()) / 2):\n",
    "    plt.axvline(line, ls = ':', color='grey', linestyle='--')\n",
    "\n",
    "plt.ylabel(metric)\n",
    "plt.xticks(rotation = 0)\n",
    "plt.grid(alpha = 0.5)\n",
    "plt.legend(bbox_to_anchor=(1.0, 1.0), loc='upper left')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_p = pd.concat(evaluation_p)\n",
    "all_p.index.rename('Top p %', level = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = all_p.groupby(['Top p %', 'Metric', 'Outcome']).mean()\n",
    "std = all_p.groupby(['Top p %', 'Metric', 'Outcome']).std()\n",
    "\n",
    "selection, ax = mean.index.get_level_values('Metric') == metric, None\n",
    "for model, color in zip(mean.columns, colors): \n",
    "    mean_model = mean[model][selection].droplevel('Metric').unstack('Outcome')\n",
    "    std_model = std[model][selection].droplevel('Metric').unstack('Outcome')\n",
    "    ax = mean_model.plot(subplots = True, ax = ax, yerr = std_model, layout=(1, len(hue)), legend = False, sharey = True, figsize = (14,5), color = color)\n",
    "\n",
    "for ax_outcome, outcome in zip(ax, mean_model.columns):\n",
    "    ax_outcome.set_title(outcome)\n",
    "\n",
    "ax[0].set_ylabel(metric)\n",
    "ax[-1].legend(ax[-1].lines, mean.columns, bbox_to_anchor=(1.0, 1.0), loc='upper left')\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
