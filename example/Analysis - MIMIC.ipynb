{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "matplotlib.rcParams['mathtext.fontset'] = 'cm'\n",
    "matplotlib.rcParams.update({'font.size': 15})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"../results/mimic_4bis_mlp_rho=0.05_p1=6_p2=0.35_p3=0.002.pkl\"\n",
    "dataset = \"../data/triage_scenario_{}.csv\".format(result_file[result_file.index('_')+1:result_file.index('mlp')-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(dataset, index_col = [0, 1])\n",
    "covariates, target = data.drop(columns = ['D', 'Y1', 'Y2', 'YC', 'nurse']), data[['D', 'Y1', 'YC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pickle.load(open(result_file, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(results, target, p):\n",
    "    \"\"\"\n",
    "        Evaluate dictionary of results\n",
    "\n",
    "    Args:\n",
    "        results (_type_): _description_\n",
    "        target ()\n",
    "        p (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: Dictionary of results\n",
    "    \"\"\"\n",
    "    evaluation = {}\n",
    "    \n",
    "    for i, result in enumerate(results): \n",
    "        eval = {\n",
    "            '$f_Y$': compute_metrics(result['Observed'], target, p),\n",
    "            '$f_h$': compute_metrics(result['Human'], target, p),\n",
    "            '$f_\\mathcal{A}$': compute_metrics(result['Amalgamation'], target, p),\n",
    "            '$f_{hyb}$': compute_metrics(result['Hybrid'], target, p),\n",
    "            '$f_{def}$': compute_metrics(result['Defer'], target, p),\n",
    "        }\n",
    "        try: eval['Observed Negative Outcome'] = {(tar, 'TNR'): (1 - target[tar].loc[results[0].index][covariates.loc[results[0].index].anchor_age == 1]).mean() for tar in target.columns}\n",
    "        except: pass\n",
    "        evaluation[i] = pd.DataFrame.from_dict(eval)\n",
    "    \n",
    "    evaluation = pd.concat(evaluation)\n",
    "    evaluation.index.rename(['Fold', 'Outcome', 'Metric'], inplace = True)\n",
    "    return evaluation \n",
    "\n",
    "def compute_metrics(predictions, target, p):\n",
    "    metrics = {}\n",
    "    tar_test = target.loc[predictions.index]\n",
    "    for tar in target.columns:\n",
    "        metrics[(tar, 'AUC-ROC')] = roc_auc_score(tar_test[tar], predictions)\n",
    "    try:\n",
    "        bot = predictions.nsmallest(n = int(p * len(predictions)), keep = 'all').index\n",
    "        female = covariates.loc[predictions.index].Group == 1\n",
    "        bot_female = bot.intersection(female[female].index)\n",
    "        bot_male = bot.intersection(female[~female].index)\n",
    "        for tar in target.columns:\n",
    "            metrics[(tar, 'Female TNR')] = 1 - tar_test[tar].loc[bot_female].mean()\n",
    "            metrics[(tar, 'Female PNR')] = len(bot_female) / female.sum()\n",
    "            metrics[(tar, 'Male TNR')] = 1 - tar_test[tar].loc[bot_male].mean()\n",
    "            metrics[(tar, 'Male PNR')] = len(bot_male) / (~female).sum()\n",
    "    except Exception as e: pass \n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = evaluate(results, target, 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'AUC-ROC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['tab:green', 'tab:red', 'tab:blue', 'tab:orange', 'tab:brown', 'tab:grey']\n",
    "patterns = ['/', '-', '\\\\', '.', '|', '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = evaluation.groupby(['Metric', 'Outcome']).mean()\n",
    "std = evaluation.groupby(['Metric', 'Outcome']).std()\n",
    "\n",
    "ax = mean.loc[metric].dropna(axis = 1, how = 'all').plot.bar(edgecolor = 'white', width = 0.8, figsize = (10, 5), yerr = std.loc[metric].dropna(axis = 1, how = 'all'),\n",
    "                            color = colors)\n",
    "\n",
    "# Add hatch\n",
    "hue = mean.loc[metric]\n",
    "hatches = [p for p in patterns for _ in range(len(hue))]\n",
    "for i, (bar, hatch) in enumerate(zip(ax.patches, hatches)):\n",
    "    bar.set_hatch(hatch)\n",
    "\n",
    "# Add separation lines\n",
    "lines = np.array([bar.get_x() for bar in ax.patches])\n",
    "for line in lines[-len(hue):-1] + ((lines[1:len(hue)] - lines[-len(hue):-1] + bar.get_width()) / 2):\n",
    "    plt.axvline(line, ls = ':', color='grey', linestyle='--')\n",
    "\n",
    "plt.ylabel(metric)\n",
    "plt.xticks(rotation = 0)\n",
    "plt.ylim(0., 1.)\n",
    "plt.grid(alpha = 0.5)\n",
    "plt.legend(bbox_to_anchor=(1.0, 1.0), loc='upper left')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict({m: [\"{:.3f} ({:.3f})\".format(mean.loc[m].loc['YC'].loc[i], std.loc[m].loc['YC'].loc[i]) for i in mean.columns] for m in mean.index.get_level_values(0)}, columns = mean.columns, orient = 'index')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
