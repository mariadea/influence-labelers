{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "matplotlib.rcParams['mathtext.fontset'] = 'cm'\n",
    "matplotlib.rcParams.update({'font.size': 15})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_folder = \"../results/mimic_1_mlp_delta=0.05_gamma1=6_gamma2=0.95_gamma3=0.002\"\n",
    "dataset = \"../data/triage_scenario_{}.csv\".format(result_folder[result_folder.index('_')+1:result_folder.index('mlp')-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(dataset, index_col = [0, 1])\n",
    "covariates, target = data.drop(columns = ['D', 'Y1', 'Y2', 'YC', 'nurse']), data[['D', 'Y1', 'Y2', 'YC']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching = {\n",
    "    'f_Y': '$f_Y$',\n",
    "    'f_D': '$f_D$',\n",
    "    \n",
    "    'f_hyb': '$f_{hyb}$',\n",
    "    'f_ensemble': '$f_{ens}$',\n",
    "    'f_weak': '$f_{weak}$',\n",
    "    'f_robust': '$f_{noise}$',\n",
    "\n",
    "    'f_A': '$f_\\mathcal{A}$'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(target, p):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of the models in the result folder\n",
    "    \"\"\"\n",
    "    evaluation = {}\n",
    "\n",
    "    # Enumerate through folds\n",
    "    folds = os.listdir(result_folder)\n",
    "    for fold in folds:\n",
    "        eval = {}\n",
    "        file_path = os.path.join(result_folder, fold)\n",
    "        for model in os.listdir(file_path):\n",
    "            if 'f_' in model:\n",
    "                res = pd.read_csv(os.path.join(file_path, model), index_col = [0, 1])['0']\n",
    "                eval[matching[model[:model.index('.csv')]]] = compute_metrics(res, target, p)\n",
    "        evaluation[fold] = pd.DataFrame.from_dict(eval)\n",
    "\n",
    "    evaluation = pd.concat(evaluation)\n",
    "    evaluation.index.rename(['Fold', 'Outcome', 'Metric'], inplace = True)\n",
    "    return evaluation \n",
    "\n",
    "def compute_metrics(predictions, target, p):\n",
    "    metrics = {}\n",
    "    tar_test = target.loc[predictions.index]\n",
    "    for tar in target.columns:\n",
    "        metrics[(tar, 'AUC-ROC')] = roc_auc_score(tar_test[tar], predictions)\n",
    "    try:\n",
    "        bot = predictions.nsmallest(n = int(p * len(predictions)), keep = 'all').index\n",
    "        female = covariates.loc[predictions.index].Group == 1\n",
    "        bot_female = bot.intersection(female[female].index)\n",
    "        bot_male = bot.intersection(female[~female].index)\n",
    "        for tar in target.columns:\n",
    "            metrics[(tar, 'Female TNR')] = 1 - tar_test[tar].loc[bot_female].mean()\n",
    "            metrics[(tar, 'Female PNR')] = len(bot_female) / female.sum()\n",
    "            metrics[(tar, 'Male TNR')] = 1 - tar_test[tar].loc[bot_male].mean()\n",
    "            metrics[(tar, 'Male PNR')] = len(bot_male) / (~female).sum()\n",
    "    except Exception as e: pass\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = evaluate(target, 0.3)\n",
    "evaluation = evaluation[[col for col in matching.values() if col in evaluation.columns]] # Reorder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'AUC-ROC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['tab:green', 'tab:red', 'tab:blue', 'tab:orange', 'tab:brown', 'tab:grey', 'tab:purple', 'tab:olive']\n",
    "patterns = ['/', '-', '\\\\', '.', '|', '', 'x', 'o']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.columns.inter(matching.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = evaluation.groupby(['Metric', 'Outcome']).mean()\n",
    "std = evaluation.groupby(['Metric', 'Outcome']).std()\n",
    "\n",
    "ax = mean.loc[metric].dropna(axis = 1, how = 'all').plot.bar(edgecolor = 'white', width = 0.8, figsize = (10, 5), yerr = std.loc[metric].dropna(axis = 1, how = 'all'),\n",
    "                            color = colors)\n",
    "\n",
    "# Add hatch\n",
    "hue = mean.loc[metric]\n",
    "hatches = [p for p in patterns for _ in range(len(hue))]\n",
    "for i, (bar, hatch) in enumerate(zip(ax.patches, hatches)):\n",
    "    bar.set_hatch(hatch)\n",
    "\n",
    "# Add separation lines\n",
    "lines = np.array([bar.get_x() for bar in ax.patches])\n",
    "for line in lines[-len(hue):-1] + ((lines[1:len(hue)] - lines[-len(hue):-1] + bar.get_width()) / 2):\n",
    "    plt.axvline(line, ls = ':', color='grey', linestyle='--')\n",
    "\n",
    "plt.ylabel(metric)\n",
    "plt.xticks(rotation = 0)\n",
    "plt.ylim(0., 1.)\n",
    "plt.grid(alpha = 0.5)\n",
    "plt.legend(bbox_to_anchor=(1.0, 1.0), loc='upper left')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict({m: [\"{:.3f} ({:.3f})\".format(mean.loc[m].loc['YC'].loc[i], std.loc[m].loc['YC'].loc[i]) for i in mean.columns] for m in mean.index.get_level_values(0)}, columns = mean.columns, orient = 'index')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
