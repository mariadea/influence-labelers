{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "matplotlib.rcParams['mathtext.fontset'] = 'cm'\n",
    "matplotlib.rcParams.update({'font.size': 15})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"../results/mimic_3.pkl\"\n",
    "dataset = \"../data/triage_clean.csv\"\n",
    "selective = False # Selective observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'mimic' in result_file:\n",
    "    data = pd.read_csv(dataset, index_col = [0, 1])\n",
    "    covariates, target = data.drop(columns = ['D', 'Y1', 'Y2', 'YC', 'acuity', 'nurse']), data[['D', 'Y1', 'Y2', 'YC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'child' in result_file:\n",
    "    with open('../../data/ChildWelfare/X_preprocess.pkl', 'rb') as handle:\n",
    "        X, screener_ids, refer_ids, Y_obs, D, Y_serv,Y_sub,colnames = pickle.load(handle)\n",
    "\n",
    "    # Remove less than 10 observation by experts\n",
    "    drop_experts = []\n",
    "    for num in screener_ids:\n",
    "        if screener_ids.count(num) < 10:\n",
    "            drop_experts.append(num)\n",
    "\n",
    "    drop_idx = []\n",
    "    for index, elem in enumerate(screener_ids):\n",
    "        if elem in drop_experts:\n",
    "            drop_idx.append(index)\n",
    "    \n",
    "    X = np.delete(X, drop_idx, axis = 0)\n",
    "    Y_serv = np.delete(Y_serv, drop_idx, axis = 0)\n",
    "    Y_sub = np.delete(Y_sub, drop_idx, axis = 0)\n",
    "    Y_obs = np.delete(Y_obs, drop_idx, axis = 0)\n",
    "    D = np.delete(D, drop_idx, axis = 0)\n",
    "    refer_ids = np.delete(refer_ids, drop_idx, axis = 0).flatten()\n",
    "\n",
    "    D = D.reshape((D.shape[0],))\n",
    "    Y_obs = Y_obs.reshape((Y_obs.shape[0],))\n",
    "\n",
    "    target = pd.DataFrame({'D': D, 'OOH': Y_obs, 'Substantiated': Y_sub, 'Services': Y_serv,})\n",
    "    covariates = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pickle.load(open(result_file, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(results, target, p):\n",
    "    \"\"\"\n",
    "        Evaluate dictionary of results\n",
    "\n",
    "    Args:\n",
    "        results (_type_): _description_\n",
    "        target ()\n",
    "        p (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: Dictionary of results\n",
    "    \"\"\"\n",
    "    evaluation = {}\n",
    "    \n",
    "    for i, result in enumerate(results): \n",
    "        evaluation[i] = pd.DataFrame.from_dict({\n",
    "            '$f_Y$': compute_metrics(result['Observed'], target, p),\n",
    "            '$f_h$': compute_metrics(result['Human'], target, p),\n",
    "            '$f_{hyb}$': compute_metrics(result['Hybrid'], target, p),\n",
    "            '$f_\\mathcal{A}$': compute_metrics(result['Amalgamation'], target, p),\n",
    "            'Overall prev.': {(tar, 'Precision'): target[tar].loc[result.index].mean() for tar in target.columns}\n",
    "        })\n",
    "    \n",
    "    evaluation = pd.concat(evaluation)\n",
    "    evaluation.index.rename(['Fold', 'Outcome', 'Metric'], inplace = True)\n",
    "    return evaluation \n",
    "\n",
    "def compute_metrics(predictions, target, p):\n",
    "    # TODO: Is it the same when selective labels ?\n",
    "    predictions_screened = predictions[target['D'].loc[predictions.index]]\n",
    "\n",
    "    # Keep top p %\n",
    "    pred_kept = predictions_screened.nlargest(n = int(p * len(predictions_screened)), keep = 'all')\n",
    "\n",
    "    metrics = {}\n",
    "    for tar in target.columns:\n",
    "        metrics.update({\n",
    "            (tar, 'Precision'): target[tar].loc[pred_kept.index].sum() / len(pred_kept), \n",
    "            (tar, 'Recall'): target[tar].loc[pred_kept.index].sum() / len(predictions)\n",
    "        })\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_p = {p: evaluate(results, target, p) for p in [0.25]}#np.arange(0.15, 0.45, 0.05)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.25\n",
    "metric = 'Recall'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['tab:green', 'tab:red', 'tab:blue', 'tab:orange', 'tab:grey']\n",
    "patterns = ['/', '-', '\\\\', '.', '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = evaluation_p[p].groupby(['Metric', 'Outcome']).mean()\n",
    "std = evaluation_p[p].groupby(['Metric', 'Outcome']).mean()\n",
    "\n",
    "ax = mean.loc[metric].plot.bar(edgecolor='white', linewidth = 3, \n",
    "                            width = 0.8, figsize = (10, 5), yerr = std,\n",
    "                            color = colors)\n",
    "\n",
    "# Add hatch\n",
    "hue = mean.loc[metric]\n",
    "hatches = [p for p in patterns for _ in range(len(hue))]\n",
    "for i, (bar, hatch) in enumerate(zip(ax.patches, hatches)):\n",
    "    bar.set_hatch(hatch)\n",
    "\n",
    "# Add separation lines\n",
    "lines = np.array([bar.get_x() for bar in ax.patches])\n",
    "for line in lines[-len(hue):-1] + ((lines[1:len(hue)] - lines[-len(hue):-1] + bar.get_width()) / 2):\n",
    "    plt.axvline(line, ls = ':', color='grey', linestyle='--')\n",
    "\n",
    "plt.ylabel(metric)\n",
    "plt.legend(bbox_to_anchor=(1.0, 1.0), loc='upper left')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_p = pd.concat(evaluation_p)\n",
    "all_p.index.rename('Top p %', level = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = all_p.groupby(['Top p %', 'Metric', 'Outcome']).mean()\n",
    "std = all_p.groupby(['Top p %', 'Metric', 'Outcome']).std()\n",
    "\n",
    "selection, ax = mean.index.get_level_values('Metric') == metric, None\n",
    "for model, color in zip(mean.columns, colors): \n",
    "    mean_model = mean[model][selection].droplevel('Metric').unstack('Outcome')\n",
    "    std_model = std[model][selection].droplevel('Metric').unstack('Outcome')\n",
    "    ax = mean_model.plot(subplots = True, ax = ax, yerr = std_model, layout=(1, len(hue)), legend = False, sharey = True, figsize = (14,5), color = color)\n",
    "\n",
    "for ax_outcome, outcome in zip(ax, mean_model.columns):\n",
    "    ax_outcome.set_title(outcome)\n",
    "\n",
    "ax[0].set_ylabel(metric)\n",
    "ax[-1].legend(ax[-1].lines, mean.columns, bbox_to_anchor=(1.0, 1.0), loc='upper left')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
