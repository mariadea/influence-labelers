{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from model import *\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIMIC - Triage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook runs the analysis on the MIMIC emergency data by leveraging experts' agreement\n",
    "1. Explore a model build on data ignoring experts \n",
    "2. Compute agreement between experts using influence function\n",
    "3. Retrain the model on the set of labels for which experts strongly agree\n",
    "\n",
    "The current analysis uses multi layer perceptrons in a single train / test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = \"../data/triage_scenario_1.csv\" # Data file - Choose the scenario of interest\n",
    "selective_labels = False # Is it a case of selective labels (only observe the outcome for patients filtered by nurse: D == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reopen the data created with the notebook in `data/`\n",
    "\n",
    "To use with your data, you will need to change the following line to open a file with:\n",
    "- `X` covariates\n",
    "- `H` associated (human) experts \n",
    "- `D` their decision for each case\n",
    "- `Y` observed outcome\n",
    "- `Yc` a concept "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triage = pd.read_csv(data_set, index_col = [0, 1])\n",
    "covariates, target, nurses = triage.drop(columns = ['D', 'Y1', 'Y2', 'YC', 'nurse']), triage[['D', 'Y1', 'Y2', 'YC']], triage['nurse']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data in a 80% train, 20% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_train, cov_test, tar_train, tar_test, nur_train, nur_test = train_test_split(covariates, target, nurses, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model's characteristics\n",
    "params = {'layers': [[50]]} # If = [[]] equivalent to a simple logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predictions, p = 0.3):\n",
    "    # Overall Performances\n",
    "    print('Performance')\n",
    "    for tar in ['Y1', 'Y2', 'YC', 'D']:\n",
    "        print('{} - AUC: {:.3f}'.format(tar, roc_auc_score(tar_test[tar], predictions)))\n",
    "\n",
    "        try:\n",
    "            predictions = pd.Series(predictions, index = tar_test.index)\n",
    "            bot = predictions.nsmallest(n = int(p * len(predictions)), keep = 'all').index\n",
    "            female = covariates.loc[predictions.index].Group == 1\n",
    "            bot_female = bot.intersection(female[female].index)\n",
    "            \n",
    "            print('{} - Female TNR: {:.3f}'.format(tar, 1 - tar_test[tar].loc[bot_female].mean()))\n",
    "            print('{} - Female PNR: {:.3f}'.format(tar, len(bot_female) / female.sum()))\n",
    "            print('\\n')\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Train on decision\n",
    "\n",
    "This model models the nurse decision based on covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_D = BinaryMLP(**params)\n",
    "f_D = f_D.fit(cov_train, tar_train['D'], nur_train, platt_calibration = True)\n",
    "predictions_d = f_D.predict(cov_test)\n",
    "evaluate(predictions_d) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Agreement computation \n",
    "\n",
    "Measure of agreeability are estimated in a cross validation fashion on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fold evaluation of influences\n",
    "folds, predictions, influence = influence_cv(BinaryMLP, cov_train, tar_train['D'], nur_train, params = params, l1_penalties = [0.001, 0.01, 0.1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics agreeability\n",
    "center_metric, opposing_metric = compute_agreeability(influence, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze confident points\n",
    "delta = 0.05 # Control which point to consider from a confience point of view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_conf = (predictions > (1 - delta)) | (predictions < delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amalgation parameters\n",
    "pi_1 = 6 # Control criterion on centre mass metric\n",
    "pi_2 = 0.8 # Control criterion on opposing metric\n",
    "pi_3 = 0.002 # On flatness\n",
    "tau = 1.0  # Balance between observed and expert labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply criteria on amalgamation\n",
    "flat_influence = (np.abs(influence) > pi_3).sum(0) == 0\n",
    "high_agr = (((center_metric > pi_1) & (opposing_metric > pi_2)) | flat_influence) & high_conf\n",
    "high_agr_correct = ((predictions - tar_train['D']).abs() < delta) & high_agr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create amalgamated labels\n",
    "tar_train['Ya'] = tar_train['Y1'].astype(int)\n",
    "tar_train.loc[high_agr_correct, 'Ya'] = (1 - tau) * tar_train['Y1'][high_agr_correct] \\\n",
    "                                            + tau * tar_train['D'][high_agr_correct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_amalg = ((tar_train['D'] == 1) | high_agr_correct) if selective_labels else tar_train['D'].isin([0, 1])\n",
    "print(\"Use: {:.2f} % of data\".format(100 * index_amalg.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Updated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_A = BinaryMLP(**params)\n",
    "f_A = f_A.fit(cov_train[index_amalg], tar_train[index_amalg]['Ya'], nur_train[index_amalg])\n",
    "predictions_amal = f_A.predict(cov_test)\n",
    "evaluate(predictions_amal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Train on observed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_observed = (tar_train['D'] == 1) if selective_labels else tar_train['D'].isin([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_Y = BinaryMLP(**params)\n",
    "f_Y = f_Y.fit(cov_train[index_observed], tar_train['Y1'][index_observed], nur_train[index_observed])\n",
    "predictions_y = f_Y.predict(cov_test)\n",
    "evaluate(predictions_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Hybrid alternative\n",
    "\n",
    "- Leverage human model in the amalgamation set\n",
    "- Leverage outcome model on non amalgamation set\n",
    "\n",
    "Models need to be retrain on their respective subsets and calibrated to ensure to mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions_d.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute which test points are part of A for test set\n",
    "predictions_test, influence_test = influence_estimate(BinaryMLP, cov_train, tar_train['D'], nur_train, cov_test, params = params, l1_penalties = [0.001, 0.01, 0.1, 1])\n",
    "center_metric, opposing_metric = compute_agreeability(influence_test, predictions_test)\n",
    "flat_influence_test = (np.abs(influence_test) > pi_3).sum(0) == 0\n",
    "high_conf_test = (predictions_test > (1 - delta)) | (predictions_test < delta)\n",
    "high_agr_test = (((center_metric > pi_1) & (opposing_metric > pi_2)) | flat_influence_test) & high_conf_test\n",
    "high_agr_correct_test = ((predictions_test - tar_test['D']).abs() < delta) & high_agr_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain a model on non almagamation only and calibrate: Rely on observed\n",
    "f_hyb = BinaryMLP(**params)\n",
    "f_hyb = f_hyb.fit(cov_train[index_observed], tar_train['Y1'][index_observed], nur_train[index_observed], platt_calibration = True)\n",
    "predictions[~high_agr_correct_test] = f_hyb.predict(cov_test.loc[~high_agr_correct_test])\n",
    "evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Alternative consensus - Ensemble\n",
    "\n",
    "Instead of influence based, approximate the consensus by an ensemble model: each model is trained on one expert, then consistency is estimated by averaging the decision made across experts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions = ensemble_agreement_cv(BinaryMLP, cov_train, tar_train['D'], nur_train, params = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (decisions > 0.5).mean(0) # Take the average of the binarized decisions \n",
    "high_conf = (predictions > (1 - delta)) | (predictions < delta)\n",
    "high_agr_correct = ((predictions - tar_train['D']).abs() < delta) & high_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the differente experts decisions, estimate agreement\n",
    "tar_train['Ya_ens'] = tar_train['Y1'].astype(int)\n",
    "tar_train.loc[high_agr_correct, 'Ya_ens'] = (1 - tau) * tar_train['Y1'][high_agr_correct] \\\n",
    "                                            + tau * tar_train['D'][high_agr_correct]\n",
    "index_amalg = ((tar_train['D'] == 1) | high_agr_correct) if selective_labels else tar_train['D'].isin([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_A_ens = BinaryMLP(**params)\n",
    "f_A_ens = f_A_ens.fit(cov_train[index_amalg], tar_train[index_amalg]['Ya_ens'], nur_train[index_amalg])\n",
    "predictions_amal_ens = f_A_ens.predict(cov_test)\n",
    "evaluate(predictions_amal_ens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7. Defer approach\n",
    "\n",
    "Jointly learn when to defer to human and when to use the model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.defer import DeferMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_defer = DeferMLP(**params)\n",
    "f_defer = f_defer.fit(cov_train[index_observed], tar_train['Y1'][index_observed], tar_train['D'][index_observed])\n",
    "predictions_defer = f_defer.predict(cov_test, tar_test['D'])\n",
    "evaluate(predictions_defer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. Ensemble \n",
    "\n",
    "Train a separate model for $Y$ and $D$ and average their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate((predictions_d + predictions_y) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. Weak supervision\n",
    "\n",
    "Average the label of $Y$ and $D$ (0.5 if disagree) and train a model on theselabels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_labels = (tar_train['D'] + tar_train['Y1']).fillna(tar_train['D']) / 2 # For weak supervision, use observed decisions when no Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_weak = BinaryMLP(**params)\n",
    "f_weak = f_weak.fit(cov_train, weak_labels, nur_train)\n",
    "predictions_weak = f_weak.predict(cov_test)\n",
    "evaluate(predictions_weak) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. Noisy labels learning\n",
    "\n",
    "Use confident learning to discard noisy labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cleanlab\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the set of points with noisy labels\n",
    "f_robust = cleanlab.classification.CleanLearning(MLPClassifier(50))\n",
    "label_issues = f_robust.find_label_issues(cov_train, tar_train['D'].astype(int))\n",
    "\n",
    "# Remove data with labels issue\n",
    "selection = ~label_issues.is_label_issue.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fair comparison train the same model than other methods\n",
    "f_robust = BinaryMLP(**params)\n",
    "f_robust.fit(cov_train.iloc[selection], tar_train['D'].iloc[selection], nur_train)\n",
    "predictions_robust = f_robust.predict(cov_test)\n",
    "evaluate(predictions_robust) "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a54f3b3a447186e9a4a83057d2abe8df010acd7b8f131225203d307ef84eba48"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
