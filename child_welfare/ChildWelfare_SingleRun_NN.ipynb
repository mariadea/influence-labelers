{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from model import *\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/ChildWelfare/X_preprocess.pkl', 'rb') as handle:\n",
    "    X,screener_ids,refer_ids,Y_obs,D,Y_serv,Y_sub,colnames = pkl.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop instances if expert assessed a single case\n",
    "\n",
    "drop_experts = []\n",
    "for num in screener_ids:\n",
    "\n",
    "    if screener_ids.count(num) < 10:\n",
    "\n",
    "        drop_experts.append(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_idx = []\n",
    "for index, elem in enumerate(screener_ids):\n",
    "    if elem in drop_experts:\n",
    "        drop_idx.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.delete(X,drop_idx,axis=0)\n",
    "Y_serv = np.delete(Y_serv,drop_idx,axis=0)\n",
    "Y_sub = np.delete(Y_sub,drop_idx,axis=0)\n",
    "Y_obs = np.delete(Y_obs,drop_idx,axis=0)\n",
    "D = np.delete(D,drop_idx,axis=0)\n",
    "refer_ids = np.delete(refer_ids,drop_idx,axis=0)\n",
    "screener_ids = np.delete(screener_ids,drop_idx,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46528, 216)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = D.reshape((D.shape[0],))\n",
    "Y_obs = Y_obs.reshape((Y_obs.shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.DataFrame({'D': D, 'Y1': Y_obs, 'Y2': Y_serv, 'Y3': Y_sub})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data in a 80% train, 20% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_train, cov_test, tar_train, tar_test, nur_train, nur_test = train_test_split(pd.DataFrame(X), target, pd.Series(screener_ids), test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model's characteristics\n",
    "#[(100,), (200,), (100,50), (200,50),(200,100),(200,100,50)]\n",
    "params = {'layers': [200, 100]} # If = [] equivalent to a simple logistic regression\n",
    "\n",
    "# Amalgation parameters\n",
    "rho = 0.02 # Control which point to consider from a confience point of view\n",
    "pi_1 = 4.0 # Control criterion on centre mass metric\n",
    "pi_2 = 0.8 # Control criterion on opposing metric\n",
    "tau = 1.0  # Balance between observed and expert labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Train on decision\n",
    "\n",
    "This model models the nurse decision based on covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.428:  14%|█▎        | 137/1000 [08:20<49:49,  3.46s/it]  "
     ]
    }
   ],
   "source": [
    "for l1_penalty in [ 0.001, 0.01, 0.1]:\n",
    "    try:\n",
    "        model = BinaryMLP(**params)\n",
    "        model = model.fit(cov_train, tar_train['D'], nur_train, l1_penalty = l1_penalty)\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(e, l1_penalty)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_h_test = model.predict(cov_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6755354470952571"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive performance\n",
    "roc_auc_score(tar_test['Y1'], model.predict(cov_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8617985755262902"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Yc performance\n",
    "roc_auc_score(tar_test['Y2'], model.predict(cov_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9722197481441487"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(tar_test['Y3'], model.predict(cov_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8974737921173025"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(tar_test['D'], model.predict(cov_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Agreement computation \n",
    "\n",
    "Measure of agreeability are estimated in a cross validation fashion on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\n",
      "Loss: 0.578:   0%|          | 0/1000 [00:02<?, ?it/s]\u001b[A\n",
      "Loss: 0.578:   0%|          | 1/1000 [00:02<33:24,  2.01s/it]\u001b[A\n",
      "Loss: 0.534:   0%|          | 1/1000 [00:04<33:24,  2.01s/it]\u001b[A\n",
      "Loss: 0.534:   0%|          | 2/1000 [00:04<34:09,  2.05s/it]\u001b[A\n",
      "Loss: 0.522:   0%|          | 2/1000 [00:06<34:09,  2.05s/it]\u001b[A\n",
      "Loss: 0.522:   0%|          | 3/1000 [00:06<34:39,  2.09s/it]\u001b[A\n",
      "Loss: 0.523:   0%|          | 3/1000 [00:08<34:39,  2.09s/it]\u001b[A\n",
      "Loss: 0.523:   0%|          | 4/1000 [00:08<34:49,  2.10s/it]\u001b[A\n",
      "Loss: 0.515:   0%|          | 4/1000 [00:10<34:49,  2.10s/it]\u001b[A\n",
      "Loss: 0.515:   0%|          | 5/1000 [00:10<34:23,  2.07s/it]\u001b[A\n",
      "Loss: 0.507:   0%|          | 5/1000 [00:13<34:23,  2.07s/it]\u001b[A\n",
      "Loss: 0.507:   1%|          | 6/1000 [00:13<37:28,  2.26s/it]\u001b[A\n",
      "Loss: 0.501:   1%|          | 6/1000 [00:16<37:28,  2.26s/it]\u001b[A\n",
      "Loss: 0.501:   1%|          | 7/1000 [00:16<41:14,  2.49s/it]\u001b[A\n",
      "Loss: 0.500:   1%|          | 7/1000 [00:18<41:14,  2.49s/it]\u001b[A\n",
      "Loss: 0.500:   1%|          | 8/1000 [00:18<39:42,  2.40s/it]\u001b[A\n",
      "Loss: 0.513:   1%|          | 8/1000 [00:20<39:42,  2.40s/it]\u001b[A\n",
      "Loss: 0.513:   1%|          | 9/1000 [00:20<38:45,  2.35s/it]\u001b[A\n",
      "Loss: 0.504:   1%|          | 9/1000 [00:22<38:45,  2.35s/it]\u001b[A\n",
      "Loss: 0.504:   1%|          | 10/1000 [00:22<37:44,  2.29s/it]\u001b[A\n",
      "Loss: 0.522:   1%|          | 10/1000 [00:25<37:44,  2.29s/it]\u001b[A\n",
      "Loss: 0.522:   1%|          | 11/1000 [00:25<37:47,  2.29s/it]\u001b[A\n",
      "Loss: 0.512:   1%|          | 11/1000 [00:27<37:47,  2.29s/it]\u001b[A\n",
      "Loss: 0.512:   1%|          | 12/1000 [00:27<36:34,  2.22s/it]\u001b[A\n",
      "Loss: 0.511:   1%|          | 12/1000 [00:29<36:34,  2.22s/it]\u001b[A\n",
      "Loss: 0.511:   1%|▏         | 13/1000 [00:29<36:33,  2.22s/it]\u001b[A\n",
      "Loss: 0.519:   1%|▏         | 13/1000 [00:32<36:33,  2.22s/it]\u001b[A\n",
      "Loss: 0.519:   1%|▏         | 14/1000 [00:32<38:48,  2.36s/it]\u001b[A\n",
      "Loss: 0.507:   1%|▏         | 14/1000 [00:37<38:48,  2.36s/it]\u001b[A\n",
      "Loss: 0.507:   2%|▏         | 15/1000 [00:37<52:01,  3.17s/it]\u001b[A\n",
      "Loss: 0.513:   2%|▏         | 15/1000 [00:41<52:01,  3.17s/it]\u001b[A\n",
      "Loss: 0.513:   2%|▏         | 16/1000 [00:41<1:00:02,  3.66s/it]\u001b[A\n",
      "Loss: 0.505:   2%|▏         | 16/1000 [00:45<1:00:02,  3.66s/it]\u001b[A\n",
      "Loss: 0.505:   2%|▏         | 17/1000 [00:45<59:18,  3.62s/it]  \u001b[A\n",
      "Loss: 0.507:   2%|▏         | 17/1000 [00:49<59:18,  3.62s/it]\u001b[A\n",
      "Loss: 0.507:   2%|▏         | 18/1000 [00:49<1:01:15,  3.74s/it]\u001b[A\n",
      "Loss: 0.509:   2%|▏         | 18/1000 [00:52<1:01:15,  3.74s/it]\u001b[A\n",
      "Loss: 0.509:   2%|▏         | 19/1000 [00:52<59:40,  3.65s/it]  \u001b[A\n",
      "Loss: 0.505:   2%|▏         | 19/1000 [00:56<59:40,  3.65s/it]\u001b[A\n",
      "Loss: 0.505:   2%|▏         | 20/1000 [00:56<57:56,  3.55s/it]\u001b[A\n",
      "Loss: 0.506:   2%|▏         | 20/1000 [01:00<57:56,  3.55s/it]\u001b[A\n",
      "Loss: 0.506:   2%|▏         | 21/1000 [01:00<59:44,  3.66s/it]\u001b[A\n",
      "Loss: 0.506:   2%|▏         | 21/1000 [01:02<59:44,  3.66s/it]\u001b[A\n",
      "Loss: 0.506:   2%|▏         | 22/1000 [01:02<55:07,  3.38s/it]\u001b[A\n",
      "Loss: 0.504:   2%|▏         | 22/1000 [01:04<55:07,  3.38s/it]\u001b[A\n",
      "Loss: 0.504:   2%|▏         | 23/1000 [01:04<47:38,  2.93s/it]\u001b[A\n",
      "Loss: 0.503:   2%|▏         | 23/1000 [01:06<47:38,  2.93s/it]\u001b[A\n",
      "Loss: 0.503:   2%|▏         | 24/1000 [01:06<40:00,  2.46s/it]\u001b[A\n",
      "Loss: 0.500:   2%|▏         | 24/1000 [01:07<40:00,  2.46s/it]\u001b[A\n",
      "Loss: 0.500:   2%|▎         | 25/1000 [01:07<34:35,  2.13s/it]\u001b[A\n",
      "Loss: 0.500:   2%|▎         | 25/1000 [01:08<34:35,  2.13s/it]\u001b[A\n",
      "Loss: 0.500:   3%|▎         | 26/1000 [01:08<30:42,  1.89s/it]\u001b[A\n",
      "Loss: 0.496:   3%|▎         | 26/1000 [01:10<30:42,  1.89s/it]\u001b[A\n",
      "Loss: 0.496:   3%|▎         | 27/1000 [01:10<28:08,  1.74s/it]\u001b[A\n",
      "Loss: 0.494:   3%|▎         | 27/1000 [01:11<28:08,  1.74s/it]\u001b[A\n",
      "Loss: 0.494:   3%|▎         | 28/1000 [01:11<26:42,  1.65s/it]\u001b[A\n",
      "Loss: 0.492:   3%|▎         | 28/1000 [01:13<26:42,  1.65s/it]\u001b[A\n",
      "Loss: 0.492:   3%|▎         | 29/1000 [01:13<25:58,  1.61s/it]\u001b[A\n",
      "Loss: 0.489:   3%|▎         | 29/1000 [01:14<25:58,  1.61s/it]\u001b[A\n",
      "Loss: 0.489:   3%|▎         | 30/1000 [01:14<25:24,  1.57s/it]\u001b[A\n",
      "Loss: 0.495:   3%|▎         | 30/1000 [01:16<25:24,  1.57s/it]\u001b[A\n",
      "Loss: 0.495:   3%|▎         | 31/1000 [01:16<25:02,  1.55s/it]\u001b[A\n",
      "Loss: 0.493:   3%|▎         | 31/1000 [01:17<25:02,  1.55s/it]\u001b[A\n",
      "Loss: 0.493:   3%|▎         | 32/1000 [01:17<24:30,  1.52s/it]\u001b[A\n",
      "Loss: 0.490:   3%|▎         | 32/1000 [01:19<24:30,  1.52s/it]\u001b[A\n",
      "Loss: 0.490:   3%|▎         | 33/1000 [01:19<24:17,  1.51s/it]\u001b[A\n",
      "Loss: 0.486:   3%|▎         | 33/1000 [01:20<24:17,  1.51s/it]\u001b[A\n",
      "Loss: 0.486:   3%|▎         | 34/1000 [01:20<23:55,  1.49s/it]\u001b[A\n",
      "Loss: 0.483:   3%|▎         | 34/1000 [01:21<23:55,  1.49s/it]\u001b[A\n",
      "Loss: 0.483:   4%|▎         | 35/1000 [01:21<23:42,  1.47s/it]\u001b[A\n",
      "Loss: 0.477:   4%|▎         | 35/1000 [01:23<23:42,  1.47s/it]\u001b[A\n",
      "Loss: 0.477:   4%|▎         | 36/1000 [01:23<23:38,  1.47s/it]\u001b[A\n",
      "Loss: 0.464:   4%|▎         | 36/1000 [01:24<23:38,  1.47s/it]\u001b[A\n",
      "Loss: 0.464:   4%|▎         | 37/1000 [01:24<23:15,  1.45s/it]\u001b[A\n",
      "Loss: 0.466:   4%|▎         | 37/1000 [01:26<23:15,  1.45s/it]\u001b[A\n",
      "Loss: 0.466:   4%|▍         | 38/1000 [01:26<23:34,  1.47s/it]\u001b[A\n",
      "Loss: 0.482:   4%|▍         | 38/1000 [01:27<23:34,  1.47s/it]\u001b[A\n",
      "Loss: 0.482:   4%|▍         | 39/1000 [01:27<23:28,  1.47s/it]\u001b[A\n",
      "Loss: 0.480:   4%|▍         | 39/1000 [01:29<23:28,  1.47s/it]\u001b[A\n",
      "Loss: 0.480:   4%|▍         | 40/1000 [01:29<23:41,  1.48s/it]\u001b[A\n",
      "Loss: 0.478:   4%|▍         | 40/1000 [01:30<23:41,  1.48s/it]\u001b[A\n",
      "Loss: 0.478:   4%|▍         | 41/1000 [01:30<23:26,  1.47s/it]\u001b[A\n",
      "Loss: 0.481:   4%|▍         | 41/1000 [01:32<23:26,  1.47s/it]\u001b[A\n",
      "Loss: 0.481:   4%|▍         | 42/1000 [01:32<23:16,  1.46s/it]\u001b[A\n",
      "Loss: 0.478:   4%|▍         | 42/1000 [01:33<23:16,  1.46s/it]\u001b[A\n",
      "Loss: 0.478:   4%|▍         | 43/1000 [01:33<23:35,  1.48s/it]\u001b[A\n",
      "Loss: 0.479:   4%|▍         | 43/1000 [01:35<23:35,  1.48s/it]\u001b[A\n",
      "Loss: 0.479:   4%|▍         | 44/1000 [01:35<23:30,  1.48s/it]\u001b[A\n",
      "Loss: 0.475:   4%|▍         | 44/1000 [01:36<23:30,  1.48s/it]\u001b[A\n",
      "Loss: 0.475:   4%|▍         | 45/1000 [01:36<24:00,  1.51s/it]\u001b[A\n",
      "Loss: 0.479:   4%|▍         | 45/1000 [01:38<24:00,  1.51s/it]\u001b[A\n",
      "Loss: 0.479:   5%|▍         | 46/1000 [01:38<25:13,  1.59s/it]\u001b[A\n",
      "Loss: 0.477:   5%|▍         | 46/1000 [01:40<25:13,  1.59s/it]\u001b[A\n",
      "Loss: 0.477:   5%|▍         | 47/1000 [01:40<25:55,  1.63s/it]\u001b[A\n",
      "Loss: 0.474:   5%|▍         | 47/1000 [01:41<25:55,  1.63s/it]\u001b[A\n",
      "Loss: 0.474:   5%|▍         | 48/1000 [01:41<26:37,  1.68s/it]\u001b[A\n",
      "Loss: 0.475:   5%|▍         | 48/1000 [01:43<26:37,  1.68s/it]\u001b[A\n",
      "Loss: 0.475:   5%|▍         | 49/1000 [01:43<26:49,  1.69s/it]\u001b[A\n",
      "Loss: 0.472:   5%|▍         | 49/1000 [01:45<26:49,  1.69s/it]\u001b[A\n",
      "Loss: 0.472:   5%|▌         | 50/1000 [01:45<26:48,  1.69s/it]\u001b[A\n",
      "Loss: 0.474:   5%|▌         | 50/1000 [01:47<26:48,  1.69s/it]\u001b[A\n",
      "Loss: 0.474:   5%|▌         | 51/1000 [01:47<26:50,  1.70s/it]\u001b[A\n",
      "Loss: 0.447:   5%|▌         | 51/1000 [01:48<26:50,  1.70s/it]\u001b[A\n",
      "Loss: 0.447:   5%|▌         | 52/1000 [01:48<26:30,  1.68s/it]\u001b[A\n",
      "Loss: 0.445:   5%|▌         | 52/1000 [01:50<26:30,  1.68s/it]\u001b[A\n",
      "Loss: 0.445:   5%|▌         | 53/1000 [01:50<26:30,  1.68s/it]\u001b[A\n",
      "Loss: 0.448:   5%|▌         | 53/1000 [01:52<26:30,  1.68s/it]\u001b[A\n",
      "Loss: 0.448:   5%|▌         | 54/1000 [01:52<26:42,  1.69s/it]\u001b[A\n",
      "Loss: 0.449:   5%|▌         | 54/1000 [01:53<26:42,  1.69s/it]\u001b[A\n",
      "Loss: 0.449:   6%|▌         | 55/1000 [01:53<26:17,  1.67s/it]\u001b[A\n",
      "Loss: 0.452:   6%|▌         | 55/1000 [01:55<26:17,  1.67s/it]\u001b[A\n",
      "Loss: 0.452:   6%|▌         | 56/1000 [01:55<27:36,  1.76s/it]\u001b[A\n",
      "Loss: 0.451:   6%|▌         | 56/1000 [01:58<27:36,  1.76s/it]\u001b[A\n",
      "Loss: 0.451:   6%|▌         | 57/1000 [01:58<30:00,  1.91s/it]\u001b[A\n",
      "Loss: 0.470:   6%|▌         | 57/1000 [01:59<30:00,  1.91s/it]\u001b[A\n",
      "Loss: 0.470:   6%|▌         | 58/1000 [01:59<30:05,  1.92s/it]\u001b[A\n",
      "Loss: 0.461:   6%|▌         | 58/1000 [02:02<30:05,  1.92s/it]\u001b[A\n",
      "Loss: 0.461:   6%|▌         | 59/1000 [02:02<30:57,  1.97s/it]\u001b[A\n",
      "Loss: 0.464:   6%|▌         | 59/1000 [02:04<30:57,  1.97s/it]\u001b[A\n",
      "Loss: 0.464:   6%|▌         | 60/1000 [02:04<31:43,  2.03s/it]\u001b[A\n",
      "Loss: 0.464:   6%|▌         | 60/1000 [02:06<31:43,  2.03s/it]\u001b[A\n",
      "Loss: 0.464:   6%|▌         | 61/1000 [02:06<33:43,  2.15s/it]\u001b[A\n",
      "Loss: 0.465:   6%|▌         | 61/1000 [02:08<33:43,  2.15s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.465:   6%|▌         | 62/1000 [02:08<34:19,  2.20s/it]\u001b[A\n",
      "Loss: 0.462:   6%|▌         | 62/1000 [02:11<34:19,  2.20s/it]\u001b[A\n",
      "Loss: 0.462:   6%|▋         | 63/1000 [02:11<33:49,  2.17s/it]\u001b[A\n",
      "Loss: 0.449:   6%|▋         | 63/1000 [02:12<33:49,  2.17s/it]\u001b[A\n",
      "Loss: 0.449:   6%|▋         | 64/1000 [02:12<31:58,  2.05s/it]\u001b[A\n",
      "Loss: 0.444:   6%|▋         | 64/1000 [02:14<31:58,  2.05s/it]\u001b[A\n",
      "Loss: 0.444:   6%|▋         | 65/1000 [02:14<32:23,  2.08s/it]\u001b[A\n",
      "Loss: 0.447:   6%|▋         | 65/1000 [02:17<32:23,  2.08s/it]\u001b[A\n",
      "Loss: 0.447:   7%|▋         | 66/1000 [02:17<32:44,  2.10s/it]\u001b[A\n",
      "Loss: 0.445:   7%|▋         | 66/1000 [02:19<32:44,  2.10s/it]\u001b[A\n",
      "Loss: 0.445:   7%|▋         | 67/1000 [02:19<32:56,  2.12s/it]\u001b[A\n",
      "Loss: 0.446:   7%|▋         | 67/1000 [02:21<32:56,  2.12s/it]\u001b[A\n",
      "Loss: 0.446:   7%|▋         | 68/1000 [02:21<31:26,  2.02s/it]\u001b[A\n",
      "Loss: 0.445:   7%|▋         | 68/1000 [02:23<31:26,  2.02s/it]\u001b[A\n",
      "Loss: 0.445:   7%|▋         | 69/1000 [02:23<33:30,  2.16s/it]\u001b[A\n",
      "Loss: 0.441:   7%|▋         | 69/1000 [02:26<33:30,  2.16s/it]\u001b[A\n",
      "Loss: 0.441:   7%|▋         | 70/1000 [02:26<36:05,  2.33s/it]\u001b[A\n",
      "Loss: 0.440:   7%|▋         | 70/1000 [02:28<36:05,  2.33s/it]\u001b[A\n",
      "Loss: 0.440:   7%|▋         | 71/1000 [02:28<36:50,  2.38s/it]\u001b[A\n",
      "Loss: 0.440:   7%|▋         | 71/1000 [02:31<36:50,  2.38s/it]\u001b[A\n",
      "Loss: 0.440:   7%|▋         | 72/1000 [02:31<36:41,  2.37s/it]\u001b[A\n",
      "Loss: 0.439:   7%|▋         | 72/1000 [02:33<36:41,  2.37s/it]\u001b[A\n",
      "Loss: 0.439:   7%|▋         | 73/1000 [02:33<36:36,  2.37s/it]\u001b[A\n",
      "Loss: 0.437:   7%|▋         | 73/1000 [02:35<36:36,  2.37s/it]\u001b[A\n",
      "Loss: 0.437:   7%|▋         | 74/1000 [02:35<36:27,  2.36s/it]\u001b[A\n",
      "Loss: 0.446:   7%|▋         | 74/1000 [02:38<36:27,  2.36s/it]\u001b[A\n",
      "Loss: 0.446:   8%|▊         | 75/1000 [02:38<36:10,  2.35s/it]\u001b[A\n",
      "Loss: 0.440:   8%|▊         | 75/1000 [02:41<36:10,  2.35s/it]\u001b[A\n",
      "Loss: 0.440:   8%|▊         | 76/1000 [02:41<38:57,  2.53s/it]\u001b[A\n",
      "Loss: 0.440:   8%|▊         | 76/1000 [02:45<38:57,  2.53s/it]\u001b[A\n",
      "Loss: 0.440:   8%|▊         | 77/1000 [02:45<48:47,  3.17s/it]\u001b[A\n",
      "Loss: 0.435:   8%|▊         | 77/1000 [02:50<48:47,  3.17s/it]\u001b[A\n",
      "Loss: 0.435:   8%|▊         | 78/1000 [02:50<57:43,  3.76s/it]\u001b[A\n",
      "Loss: 0.438:   8%|▊         | 78/1000 [02:55<57:43,  3.76s/it]\u001b[A\n",
      "Loss: 0.438:   8%|▊         | 79/1000 [02:55<1:00:51,  3.97s/it]\u001b[A\n",
      "Loss: 0.437:   8%|▊         | 79/1000 [02:59<1:00:51,  3.97s/it]\u001b[A\n",
      "Loss: 0.437:   8%|▊         | 80/1000 [02:59<1:02:00,  4.04s/it]\u001b[A\n",
      "Loss: 0.442:   8%|▊         | 80/1000 [03:02<1:02:00,  4.04s/it]\u001b[A\n",
      "Loss: 0.442:   8%|▊         | 81/1000 [03:02<58:27,  3.82s/it]  \u001b[A\n",
      "Loss: 0.436:   8%|▊         | 81/1000 [03:05<58:27,  3.82s/it]\u001b[A\n",
      "Loss: 0.436:   8%|▊         | 82/1000 [03:05<53:22,  3.49s/it]\u001b[A\n",
      "Loss: 0.435:   8%|▊         | 82/1000 [03:08<53:22,  3.49s/it]\u001b[A\n",
      "Loss: 0.435:   8%|▊         | 83/1000 [03:08<51:44,  3.39s/it]\u001b[A\n",
      "Loss: 0.428:   8%|▊         | 83/1000 [03:12<51:44,  3.39s/it]\u001b[A\n",
      "Loss: 0.428:   8%|▊         | 84/1000 [03:12<51:37,  3.38s/it]\u001b[A\n",
      "Loss: 0.431:   8%|▊         | 84/1000 [03:15<51:37,  3.38s/it]\u001b[A\n",
      "Loss: 0.431:   8%|▊         | 85/1000 [03:15<49:37,  3.25s/it]\u001b[A\n",
      "Loss: 0.431:   8%|▊         | 85/1000 [03:17<49:37,  3.25s/it]\u001b[A\n",
      "Loss: 0.431:   9%|▊         | 86/1000 [03:17<44:17,  2.91s/it]\u001b[A\n",
      "Loss: 0.434:   9%|▊         | 86/1000 [03:18<44:17,  2.91s/it]\u001b[A\n",
      "Loss: 0.434:   9%|▊         | 87/1000 [03:18<39:10,  2.57s/it]\u001b[A\n",
      "Loss: 0.439:   9%|▊         | 87/1000 [03:21<39:10,  2.57s/it]\u001b[A"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Architecture leads to singular weights matrix for last layer: Use another architecture or increase l1_penalty.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/FairML/Influence/repos/influence-labelers/model/neural_api.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, h, vsize, val, l1_penalty, random_state, **args)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: inverse_cpu: The diagonal element 59 is zero, the inversion could not be completed because the input matrix is singular.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-36dab74a11ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fold evaluation of influences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfluence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfluence_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBinaryMLP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'D'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnur_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"l1_penalty\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ml1_penalty\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/FairML/Influence/repos/influence-labelers/model/amalgamation.py\u001b[0m in \u001b[0;36minfluence_cv\u001b[0;34m(model, x, y, h, params, fit_params, n_split)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# Train model on the subset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mmodel_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mmodel_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# Calibrate NN on validation set - Platt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/FairML/Influence/repos/influence-labelers/model/neural_api.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, h, vsize, val, l1_penalty, random_state, **args)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Architecture leads to singular weights matrix for last layer: Use another architecture or increase l1_penalty.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Architecture leads to singular weights matrix for last layer: Use another architecture or increase l1_penalty."
     ]
    }
   ],
   "source": [
    "# Fold evaluation of influences\n",
    "folds, predictions, influence = influence_cv(BinaryMLP, cov_train, tar_train['D'], nur_train, params = params, fit_params = {\"l1_penalty\": l1_penalty})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics agreeability\n",
    "center_metric, opposing_metric = compute_agreeability(influence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(center_metric, opposing_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply criteria on amalgamation\n",
    "high_conf = (predictions > (1 - rho))# | (predictions < rho)\n",
    "high_agr = (center_metric > pi_1) & (opposing_metric > pi_2) & high_conf\n",
    "high_agr_correct = ((predictions - tar_train['D']).abs() < rho) & high_agr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('high_conf:', sum(high_conf))\n",
    "print('high_agr:', sum(high_agr))\n",
    "print('high_agr_correct:', sum(high_agr_correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "prob_true, prob_pred = calibration_curve(tar_train['D'], predictions, n_bins=7)\n",
    "plt.plot(prob_true,prob_pred, marker='o', linewidth=1, label='logreg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create amalgamated labels\n",
    "tar_train['Ya'] = tar_train['Y1'].copy()\n",
    "tar_train['Ya'][high_agr_correct] = (1 - tau) * tar_train['Y1'][high_agr_correct] \\\n",
    "                                    + tau * tar_train['D'][high_agr_correct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(tar_train['D']!=tar_train['Y1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(tar_train['Ya']!=tar_train['Y1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index for selective labels\n",
    "index_amalg = [i==1.0 for i in tar_train['D']] | high_agr_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Updated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model with selective labels\n",
    "model = BinaryMLP(**params)\n",
    "model = model.fit(cov_train[index_amalg], tar_train[index_amalg]['Ya'], nur_train[index_amalg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_amalg_test = model.predict(cov_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #model without selective labels\n",
    "# model = BinaryMLP(**params)\n",
    "# model = model.fit(cov_train, tar_train['Ya'], nur_train[index_amalg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive performance\n",
    "roc_auc_score(tar_test['Y1'], model.predict(cov_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yc performance\n",
    "roc_auc_score(tar_test['Y2'],model.predict(cov_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(tar_test['Y3'],model.predict(cov_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(tar_test['D'],model.predict(cov_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Train on observed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BinaryMLP(**params)\n",
    "model = model.fit(cov_train[tar_train['D']==1], tar_train['Y1'][tar_train['D']==1], nur_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_obs_test = model.predict(cov_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive performance\n",
    "roc_auc_score(tar_test['Y1'], model.predict(cov_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yc performance\n",
    "roc_auc_score(tar_test['Y2'],model.predict(cov_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(tar_test['Y3'],model.predict(cov_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(tar_test['D'],model.predict(cov_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#if models trained without 1 penalty, retrain f_h\n",
    "model = BinaryMLP(**params)\n",
    "model = model.fit(cov_train, tar_train['D'], nur_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_h_test = model.predict(cov_test)\n",
    "\n",
    "print(roc_auc_score(tar_test['Y1'], model.predict(cov_test)))\n",
    "\n",
    "print(roc_auc_score(tar_test['Y2'], model.predict(cov_test)))\n",
    "\n",
    "print(roc_auc_score(tar_test['Y3'], model.predict(cov_test)))\n",
    "\n",
    "print(roc_auc_score(tar_test['D'], model.predict(cov_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_agr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_pred_hybrid_test = np.copy(Y_pred_obs_test)\n",
    "# Y_pred_hybrid_test[high_agr_test] = Y_pred_h_test[high_agr_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision/Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_pred_outcome(Y_outcome_test, Y_pred_test, Y_h_test, p):\n",
    "    Y_pred_test_scrin = Y_pred_test[Y_h_test==1]\n",
    "    Y_outcome_test_scrin = Y_outcome_test[Y_h_test==1]\n",
    "    #print(sum(Y_outcome_test_scrin))\n",
    "    idx_25qrtl_pred = np.argsort(Y_pred_test_scrin)[::-1][:int(np.floor(p*len(Y_pred_test_scrin)))]\n",
    "    #baseline\n",
    "    baseline = sum(Y_outcome_test_scrin)/len(Y_outcome_test_scrin)\n",
    "    #print(idx_25qrtl_pred )\n",
    "    #print(sum(Y_outcome_test_scrin[idx_25qrtl_pred]))\n",
    "    #precision and recall in top p% highsest scored screen-in\n",
    "    precision_p = sum(Y_outcome_test_scrin[idx_25qrtl_pred])/len(idx_25qrtl_pred)\n",
    "    recall_p = sum(Y_outcome_test_scrin[idx_25qrtl_pred])/sum(Y_outcome_test)\n",
    "    \n",
    "    return baseline, precision_p, recall_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Eval resp. Y_obs')\n",
    "print('f_y', eval_pred_outcome(np.array(tar_test['Y1']).flatten(), Y_pred_obs_test.flatten(), np.array(tar_test['D']==1).flatten(), 0.25))\n",
    "print('f_A', eval_pred_outcome(np.array(tar_test['Y1']).flatten(), Y_pred_amalg_test.flatten(), np.array(tar_test['D']==1).flatten(), 0.25))\n",
    "print('f_h', eval_pred_outcome(np.array(tar_test['Y1']).flatten(), Y_pred_h_test.flatten(), np.array(tar_test['D']==1).flatten(), 0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Eval resp. Y_serv')\n",
    "print('f_y', eval_pred_outcome(np.array(tar_test['Y2']).flatten(), Y_pred_obs_test.flatten(), np.array(tar_test['D']==1).flatten(), 0.25))\n",
    "print('f_A', eval_pred_outcome(np.array(tar_test['Y2']).flatten(), Y_pred_amalg_test.flatten(), np.array(tar_test['D']==1).flatten(), 0.25))\n",
    "print('f_h', eval_pred_outcome(np.array(tar_test['Y2']).flatten(), Y_pred_h_test.flatten(), np.array(tar_test['D']==1).flatten(), 0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Eval resp. Y_sub')\n",
    "print('f_y', eval_pred_outcome(np.array(tar_test['Y3']).flatten(), Y_pred_obs_test.flatten(), np.array(tar_test['D']==1).flatten(), 0.25))\n",
    "print('f_A', eval_pred_outcome(np.array(tar_test['Y3']).flatten(), Y_pred_amalg_test.flatten(), np.array(tar_test['D']==1).flatten(), 0.25))\n",
    "print('f_h', eval_pred_outcome(np.array(tar_test['Y3']).flatten(), Y_pred_h_test.flatten(), np.array(tar_test['D']==1).flatten(), 0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_pred(Y_pred_test, tar_test, p):\n",
    "    dic_k_Y_pred = {}\n",
    "    \n",
    "    base_ooh, precision_ooh, recall_ooh = eval_pred_outcome(np.array(tar_test['Y1']).flatten(), Y_pred_test.flatten(), np.array(tar_test['D']==1).flatten(), p)\n",
    "    base_serv, precision_serv, recall_serv = eval_pred_outcome(np.array(tar_test['Y2']).flatten(), Y_pred_test.flatten(), np.array(tar_test['D']==1).flatten(), p)\n",
    "    base_sub, precision_sub, recall_sub = eval_pred_outcome(np.array(tar_test['Y3']).flatten(), Y_pred_test.flatten(), np.array(tar_test['D']==1).flatten(), p)\n",
    "    \n",
    "    dic_k_Y_pred['ooh'] = {'base': float(base_ooh), 'precision': float(precision_ooh), 'recall': float(recall_ooh)}\n",
    "    dic_k_Y_pred['sub'] = {'base': float(base_sub), 'precision': float(precision_sub), 'recall': float(recall_sub)}\n",
    "    dic_k_Y_pred['serv'] = {'base': float(base_serv), 'precision': float(precision_serv), 'recall': float(recall_serv)}\n",
    "    \n",
    "    return dic_k_Y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_k_iter(Y_pred_obs_test, Y_pred_amalg_test, Y_pred_h_test, tar_test, p):\n",
    "    dic_eval = {}\n",
    "    \n",
    "    dic_k = {}\n",
    "    dic_k['Y_pred_obs_test'] = eval_pred(Y_pred_obs_test, tar_test, p)\n",
    "    dic_k['Y_pred_h_test'] = eval_pred(Y_pred_h_test, tar_test, p)\n",
    "    #dic_k['Y_pred_hybrid_test'] = eval_pred(Y_pred_hybrid_test, Y_h, Y_ooh, Y_serv, Y_sub, refer_ids_test, refer_ids, p)\n",
    "    #print('done with Y_pred_hyb_test')\n",
    "    dic_k['Y_pred_amalg_test'] = eval_pred(Y_pred_amalg_test, tar_test, p)\n",
    "\n",
    "    \n",
    "    dic_eval[0] = dic_k\n",
    "    \n",
    "    return dic_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dics =[]\n",
    "for p in np.arange(0.05, 0.4, 0.05):\n",
    "    list_dics.append(eval_k_iter(Y_pred_obs_test, Y_pred_amalg_test, Y_pred_h_test, tar_test, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_25 = eval_k_iter(Y_pred_obs_test, Y_pred_amalg_test, Y_pred_h_test, tar_test, 0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_perf_bar(dic_eval, models = ['Y_pred_obs_test', 'Y_pred_h_test',\n",
    "                                     'Y_pred_hybrid_test', 'Y_pred_amalg_test'], \n",
    "                  plot_labels = [ '$f_y$', '$f_h$', '$f_\\mathcal{hyb}$',  '$f_\\mathcal{A}$' ] ,\n",
    "                  labels = ['ooh', 'sub', 'serv'], y_max=1.05, legend_appear=True,\n",
    "                  metric ='precision', fig_title='fig_noname'):\n",
    "    \n",
    "    K = len(dic_eval.keys())\n",
    "    \n",
    "    n_labels = len(labels)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10,5))\n",
    "    index = np.arange(n_labels)\n",
    "    bar_width = 0.15\n",
    "    opacity = 0.8\n",
    "    colors = ['green', 'darkred', 'yellow', 'blue']\n",
    "    hatches = ['//', '-', '\\\\', '.']\n",
    "    \n",
    "    \n",
    "    m_idx =0\n",
    "    for m in models:\n",
    "        plot_mean = [0]*n_labels\n",
    "        plot_std = [0]*n_labels\n",
    "        \n",
    "        l_idx = 0\n",
    "        for l in labels:\n",
    "            metric_list = [0]*K\n",
    "            if metric == 'precision':\n",
    "                for k in range(K):\n",
    "                    metric_list[k] = dic_eval[k][m][l]['precision']\n",
    "            \n",
    "            if metric == 'recall':\n",
    "                for k in range(K):\n",
    "                    metric_list[k] = dic_eval[k][m][l]['recall']\n",
    "            plot_mean[l_idx] = np.mean(metric_list)  \n",
    "            plot_std[l_idx] = np.std(metric_list)  \n",
    "            l_idx += 1\n",
    "        rects = plt.bar(index+m_idx*bar_width, plot_mean, yerr= plot_std,\n",
    "            alpha=opacity, color=colors[m_idx], capsize=7,  width=.1, hatch = hatches[m_idx],\n",
    "                            label=plot_labels[m_idx])        \n",
    "            \n",
    "            \n",
    "        m_idx+=1                \n",
    "    \n",
    "    \n",
    "    for n in index[:-1]:\n",
    "        if metric=='precision':\n",
    "            t= 0.8\n",
    "        else:\n",
    "            t=0.7\n",
    "        plt.axvline(x=n+t, color='grey', linestyle='--')\n",
    "        \n",
    "    if metric=='precision':\n",
    "        plot_mean = [0]*n_labels\n",
    "        plot_std = [0]*n_labels\n",
    "        l_idx = 0\n",
    "        for l in labels:\n",
    "            metric_list = [0]*K\n",
    "            for k in range(K):\n",
    "                metric_list[k] = dic_eval[k][m][l]['base']\n",
    "            plot_mean[l_idx] = np.mean(metric_list)  \n",
    "            plot_std[l_idx] = np.std(metric_list) \n",
    "            l_idx +=1\n",
    "        rects = plt.bar(index+len(models)*bar_width, plot_mean, yerr= plot_std,\n",
    "            alpha=opacity, color='grey',capsize=7,  width=.1,label='Overall prev.')     \n",
    "\n",
    "    plt.ylabel(metric,fontsize=25)\n",
    "    #plt.ylim((0,1.05))\n",
    "    #plt.title('Performance of outcomes by model',fontsize=18)\n",
    "    plt.xticks(index + 1.5*bar_width, ('OOH', 'Substantiated', 'Services'),fontsize=18)\n",
    "    plt.yticks(fontsize=16)\n",
    "    if legend_appear==True:\n",
    "        plt.legend(loc = 'upper left', fontsize=18)\n",
    "    plt.ylim(0.0,y_max)\n",
    "    plt.tight_layout()\n",
    "\n",
    "#     plt.savefig(fig_title, dpi=300,\n",
    "#             orientation='portrait' )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_perf_bar(dic_25, models = ['Y_pred_obs_test', 'Y_pred_h_test', 'Y_pred_amalg_test'], \n",
    "                  plot_labels = [ '$f_y$', '$f_h$',   '$f_\\mathcal{A}$' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_perf_bar(list_dics, index = np.arange(0.05, 0.4, 0.05), models = ['Y_pred_obs_test', 'Y_pred_h_test',\n",
    "                                     'Y_pred_hybrid_test', 'Y_pred_amalg_test'], \n",
    "                  plot_labels = [ '$f_y$', '$f_h$', '$f_\\mathcal{hyb}$',  '$f_\\mathcal{A}$' ] ,\n",
    "                  labels = ['ooh', 'sub', 'serv'], y_max=1.05, legend_appear=True,\n",
    "                  metric ='precision', fig_title='fig_noname'):\n",
    "    \n",
    "    K = len(list_dics[0].keys())\n",
    "    \n",
    "    n_groups = len(labels)\n",
    "    \n",
    "    fig, ax_list = plt.subplots(1,n_groups, figsize=(14,5))\n",
    "    N=len(list_dics)\n",
    "    \n",
    "    bar_width = 0.1\n",
    "    opacity = 0.8\n",
    "    colors = ['green', 'darkred', 'goldenrod', 'blue']\n",
    "    hatches = ['//', '-', '\\\\', '.']\n",
    "    l_idx = 0\n",
    "    for l in labels:\n",
    "        m_idx =0\n",
    "        for m in models:\n",
    "            plot_mean = [0]*N\n",
    "            plot_std = [0]*N\n",
    "\n",
    "            dic_idx = 0\n",
    "            for dic_eval in list_dics:\n",
    "                metric_list = [0]*K\n",
    "                if metric == 'precision':\n",
    "                    for k in range(K):\n",
    "                        metric_list[k] = dic_eval[k][m][l]['precision']\n",
    "\n",
    "                if metric == 'recall':\n",
    "                    for k in range(K):\n",
    "                        metric_list[k] = dic_eval[k][m][l]['recall']\n",
    "                plot_mean[dic_idx] = np.mean(metric_list)  \n",
    "                plot_std[dic_idx] = np.std(metric_list)  \n",
    "                dic_idx += 1\n",
    "            ax_list[l_idx].errorbar(index, plot_mean, yerr= plot_std,\n",
    "                alpha=opacity, color=colors[m_idx], capsize=7, \n",
    "                               label=plot_labels[m_idx])        \n",
    "#             else:\n",
    "#                 ax_list[l_idx].errorbar(index, plot_mean, yerr= plot_std,\n",
    "#                 alpha=opacity, color=colors[m_idx], capsize=7)     \n",
    "\n",
    "            m_idx+=1                \n",
    "        \n",
    "\n",
    "#         for n in index[:-1]:\n",
    "#             if metric=='precision':\n",
    "#                 t= (len(list_dics)-1)*bar_width+0.2\n",
    "#             else:\n",
    "#                 t= len(list_dics)*bar_width\n",
    "#             plt.axvline(x=n+t, color='grey', linestyle='--')\n",
    "\n",
    "        if metric=='precision':\n",
    "            plot_mean = [0]*N\n",
    "            plot_std = [0]*N\n",
    "            dic_idx = 0\n",
    "            for dic_eval in list_dics:\n",
    "                metric_list = [0]*K\n",
    "                for k in range(K):\n",
    "                    metric_list[k] = dic_eval[k][m][l]['base']\n",
    "                plot_mean[dic_idx] = np.mean(metric_list)  \n",
    "                plot_std[dic_idx] = np.std(metric_list) \n",
    "                dic_idx +=1\n",
    "            ax_list[l_idx].errorbar(index, plot_mean, yerr= plot_std,\n",
    "                alpha=opacity, color='grey',capsize=7, label='base')     \n",
    "#             else:\n",
    "#                 ax_list[l_idx].errorbar(index, plot_mean, yerr= plot_std,\n",
    "#                 alpha=opacity, color='grey',capsize=7)     \n",
    "        l_idx+=1\n",
    "        \n",
    "        \n",
    "   # plt.ylabel(metric,fontsize=25)\n",
    "    ax_list[0].set_ylabel(metric,fontsize=20)\n",
    "    ax_list[1].set_xlabel('top p%',fontsize=20)\n",
    "    \n",
    "    for k in range(n_groups):\n",
    "        ax_list[k].set_title(labels[k],fontsize=20)\n",
    "    \n",
    "    \n",
    "    ax_list[2].legend(loc='center left', bbox_to_anchor=(1, 0.5),fontsize=18)\n",
    "    \n",
    "    #ax_list[1].legend(loc='upper center', bbox_to_anchor=(0.5, -0.05),\n",
    "        #  fancybox=True, shadow=True, ncol=5)\n",
    "    \n",
    "    #plt.xticks(index+dic_idx*bar_width)\n",
    "    \n",
    "#     plt.yticks(fontsize=16)\n",
    "#     if legend_appear==True:\n",
    "#         plt.legend(loc = 'upper left', fontsize=18)\n",
    "    #plt.ylim(0.0,y_max)\n",
    "    #plt.tight_layout()\n",
    "\n",
    "#     plt.savefig(fig_title, dpi=300,\n",
    "#             orientation='portrait' )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_perf_bar(list_dics, index = np.arange(0.05, 0.4, 0.05), models = ['Y_pred_obs_test', 'Y_pred_h_test','Y_pred_amalg_test'], \n",
    "                  plot_labels = [ '$f_y$', '$f_h$',  '$f_\\mathcal{A}$' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a54f3b3a447186e9a4a83057d2abe8df010acd7b8f131225203d307ef84eba48"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
